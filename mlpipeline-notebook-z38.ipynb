{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Spark Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1574882374057_0001</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-1-62.us-east-2.compute.internal:20888/proxy/application_1574882374057_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-12-198.us-east-2.compute.internal:8042/node/containerlogs/container_1574882374057_0001_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml._\n",
      "import org.apache.spark.ml.feature._\n",
      "import org.apache.spark.ml.linalg._\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.sql._\n",
      "import org.apache.spark.sql.functions._\n",
      "s3Prefix: String = s3://\n",
      "s3BucketName: String = dalin-ml-pipeline\n",
      "dataSourcePath: String = /transformed-csv/*.csv\n",
      "sageMakerInputPrefix: String = sagemaker/trainingInput\n",
      "sageMakerOutputPrefix: String = sagemaker/trainingOutput/XGBoost\n",
      "sageMakerRoleArn: String = arn:aws:iam::263690384742:role/SparkSageMakerRole\n",
      "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7e62ffb1\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml._\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.linalg._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val s3Prefix = \"s3://\"\n",
    "val s3BucketName = \"dalin-ml-pipeline\"\n",
    "val dataSourcePath = \"/transformed-csv/*.csv\"\n",
    "\n",
    "val sageMakerInputPrefix = \"sagemaker/trainingInput\"\n",
    "val sageMakerOutputPrefix = \"sagemaker/trainingOutput/XGBoost\"\n",
    "val sageMakerRoleArn = \"arn:aws:iam::263690384742:role/SparkSageMakerRole\"\n",
    "\n",
    "val spark = SparkSession.builder.getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Load Processed Data from S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import spark.implicits._\n",
      "originalDF: org.apache.spark.sql.DataFrame = [AGE: string, AGE_NEONATE: string ... 96 more fields]\n"
     ]
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val originalDF = spark.read.format(\"csv\").\n",
    "    option(\"header\", \"true\").\n",
    "    load(s\"$s3Prefix$s3BucketName$dataSourcePath\").\n",
    "    withColumnRenamed(\"TOTCHG\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AGE: string (nullable = true)\n",
      " |-- AGE_NEONATE: string (nullable = true)\n",
      " |-- AMONTH: string (nullable = true)\n",
      " |-- AWEEKEND: string (nullable = true)\n",
      " |-- DIED: string (nullable = true)\n",
      " |-- DISCWT: string (nullable = true)\n",
      " |-- DISPUNIFORM: string (nullable = true)\n",
      " |-- DQTR: string (nullable = true)\n",
      " |-- DRG: string (nullable = true)\n",
      " |-- DRGVER: string (nullable = true)\n",
      " |-- DRG_NoPOA: string (nullable = true)\n",
      " |-- DXVER: string (nullable = true)\n",
      " |-- ELECTIVE: string (nullable = true)\n",
      " |-- FEMALE: string (nullable = true)\n",
      " |-- HCUP_ED: string (nullable = true)\n",
      " |-- HOSP_DIVISION: string (nullable = true)\n",
      " |-- HOSP_NIS: string (nullable = true)\n",
      " |-- I10_DX1: string (nullable = true)\n",
      " |-- I10_DX2: string (nullable = true)\n",
      " |-- I10_DX3: string (nullable = true)\n",
      " |-- I10_DX4: string (nullable = true)\n",
      " |-- I10_DX5: string (nullable = true)\n",
      " |-- I10_DX6: string (nullable = true)\n",
      " |-- I10_DX7: string (nullable = true)\n",
      " |-- I10_DX8: string (nullable = true)\n",
      " |-- I10_DX9: string (nullable = true)\n",
      " |-- I10_DX10: string (nullable = true)\n",
      " |-- I10_DX11: string (nullable = true)\n",
      " |-- I10_DX12: string (nullable = true)\n",
      " |-- I10_DX13: string (nullable = true)\n",
      " |-- I10_DX14: string (nullable = true)\n",
      " |-- I10_DX15: string (nullable = true)\n",
      " |-- I10_DX16: string (nullable = true)\n",
      " |-- I10_DX17: string (nullable = true)\n",
      " |-- I10_DX18: string (nullable = true)\n",
      " |-- I10_DX19: string (nullable = true)\n",
      " |-- I10_DX20: string (nullable = true)\n",
      " |-- I10_DX21: string (nullable = true)\n",
      " |-- I10_DX22: string (nullable = true)\n",
      " |-- I10_DX23: string (nullable = true)\n",
      " |-- I10_DX24: string (nullable = true)\n",
      " |-- I10_DX25: string (nullable = true)\n",
      " |-- I10_DX26: string (nullable = true)\n",
      " |-- I10_DX27: string (nullable = true)\n",
      " |-- I10_DX28: string (nullable = true)\n",
      " |-- I10_DX29: string (nullable = true)\n",
      " |-- I10_DX30: string (nullable = true)\n",
      " |-- I10_ECAUSE1: string (nullable = true)\n",
      " |-- I10_ECAUSE2: string (nullable = true)\n",
      " |-- I10_ECAUSE3: string (nullable = true)\n",
      " |-- I10_ECAUSE4: string (nullable = true)\n",
      " |-- I10_NDX: string (nullable = true)\n",
      " |-- I10_NECAUSE: string (nullable = true)\n",
      " |-- I10_NPR: string (nullable = true)\n",
      " |-- I10_PR1: string (nullable = true)\n",
      " |-- I10_PR2: string (nullable = true)\n",
      " |-- I10_PR3: string (nullable = true)\n",
      " |-- I10_PR4: string (nullable = true)\n",
      " |-- I10_PR5: string (nullable = true)\n",
      " |-- I10_PR6: string (nullable = true)\n",
      " |-- I10_PR7: string (nullable = true)\n",
      " |-- I10_PR8: string (nullable = true)\n",
      " |-- I10_PR9: string (nullable = true)\n",
      " |-- I10_PR10: string (nullable = true)\n",
      " |-- I10_PR11: string (nullable = true)\n",
      " |-- I10_PR12: string (nullable = true)\n",
      " |-- I10_PR13: string (nullable = true)\n",
      " |-- I10_PR14: string (nullable = true)\n",
      " |-- I10_PR15: string (nullable = true)\n",
      " |-- KEY_NIS: string (nullable = true)\n",
      " |-- LOS: string (nullable = true)\n",
      " |-- MDC: string (nullable = true)\n",
      " |-- MDC_NoPOA: string (nullable = true)\n",
      " |-- NIS_STRATUM: string (nullable = true)\n",
      " |-- PAY1: string (nullable = true)\n",
      " |-- PL_NCHS: string (nullable = true)\n",
      " |-- PRDAY1: string (nullable = true)\n",
      " |-- PRDAY2: string (nullable = true)\n",
      " |-- PRDAY3: string (nullable = true)\n",
      " |-- PRDAY4: string (nullable = true)\n",
      " |-- PRDAY5: string (nullable = true)\n",
      " |-- PRDAY6: string (nullable = true)\n",
      " |-- PRDAY7: string (nullable = true)\n",
      " |-- PRDAY8: string (nullable = true)\n",
      " |-- PRDAY9: string (nullable = true)\n",
      " |-- PRDAY10: string (nullable = true)\n",
      " |-- PRDAY11: string (nullable = true)\n",
      " |-- PRDAY12: string (nullable = true)\n",
      " |-- PRDAY13: string (nullable = true)\n",
      " |-- PRDAY14: string (nullable = true)\n",
      " |-- PRDAY15: string (nullable = true)\n",
      " |-- PRVER: string (nullable = true)\n",
      " |-- RACE: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- TRAN_IN: string (nullable = true)\n",
      " |-- TRAN_OUT: string (nullable = true)\n",
      " |-- YEAR: string (nullable = true)\n",
      " |-- ZIPINC_QRTL: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Filter Data with Single Dianosis Category and Single Procedure\n",
    "> <b>ICD-10-CM</b> refers to International Classification of Diseases, 10th Revision, Clinical Modification provided by the Centers for Medicare and Medicaid Services and the National Center for Health Statistics, for medical coding and reporting in the United States. (Wikipedia) <br>Similarly, <b>ICD-10-PCS</b> refers to the procedures. \n",
    "\n",
    ">The first three characters of an ICD-10 code designate the category of the diagnosis.\n",
    "\n",
    "Link to [ICD Codes](https://icd.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import spark.implicits._\n",
      "oneDiagDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [AGE: string, AGE_NEONATE: string ... 96 more fields]\n"
     ]
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val oneDiagDF = originalDF.filter($\"I10_DX2\".isNull && \n",
    "                                  $\"I10_DX1\".isNotNull && \n",
    "                                  $\"I10_PR2\".isNull && \n",
    "                                  $\"I10_PR1\".isNotNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res5: Long = 68685\n"
     ]
    }
   ],
   "source": [
    "oneDiagDF.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|I10_DX1|count|\n",
      "+-------+-----+\n",
      "|  Z3800|19258|\n",
      "|  Z3801| 7376|\n",
      "|  K3580| 4386|\n",
      "|   P599| 2203|\n",
      "|   K352| 1430|\n",
      "|  M1711| 1230|\n",
      "|  M1611| 1217|\n",
      "|   K353| 1194|\n",
      "|  M1712| 1098|\n",
      "|  M1612| 1027|\n",
      "|  K8000|  895|\n",
      "|   Q400|  512|\n",
      "|   M179|  506|\n",
      "|S42412A|  344|\n",
      "|  Z3831|  328|\n",
      "|   K810|  274|\n",
      "|   P593|  266|\n",
      "|  K8012|  222|\n",
      "|   P819|  220|\n",
      "|  K3589|  217|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oneDiagDF.select(\"I10_DX1\").groupBy(\"I10_DX1\").count.sort(desc(\"count\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Filter Data with Z38 Diagnosis Category\n",
    "### (ICD-10-CM) Z38: Liveborn infants according to place of birth and type of delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z38DF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [AGE: string, AGE_NEONATE: string ... 96 more fields]\n",
      "res7: Long = 27118\n"
     ]
    }
   ],
   "source": [
    "val z38DF = oneDiagDF.filter($\"I10_DX1\".contains(\"Z38\"))\n",
    "z38DF.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Convert and Clean Data\n",
    "### Convert all 14 numeric columns to double type and remove invalid/missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numericColumns: Array[String] = Array(label, AGE, AGE_NEONATE, AMONTH, AWEEKEND, DIED, DQTR, ELECTIVE, FEMALE, HCUP_ED, I10_NDX, I10_NECAUSE, I10_NPR, LOS)\n",
      "toDouble: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,DoubleType,Some(List(StringType, IntegerType)))\n",
      "convertedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [AGE: string, AGE_NEONATE: string ... 96 more fields]\n",
      "filteredDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [AGE: double, AGE_NEONATE: double ... 96 more fields]\n",
      "res13: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [AGE: double, AGE_NEONATE: double ... 96 more fields]\n"
     ]
    }
   ],
   "source": [
    "val numericColumns = Array(\"label\",\"AGE\",\"AGE_NEONATE\",\"AMONTH\",\"AWEEKEND\",\n",
    "                           \"DIED\",\"DQTR\",\"ELECTIVE\",\"FEMALE\",\"HCUP_ED\",\n",
    "                           \"I10_NDX\",\"I10_NECAUSE\",\"I10_NPR\",\"LOS\")\n",
    "\n",
    "val toDouble = udf((s: String, lowerLimit: Int) => {\n",
    "  if (s != null && (s forall Character.isDigit) && s.toDouble >= lowerLimit) s.toDouble else -1\n",
    "})\n",
    "\n",
    "var convertedDF = z38DF\n",
    "for (colName <- numericColumns) {\n",
    "  convertedDF = convertedDF.withColumn(colName, toDouble(col(colName), lit(0)))\n",
    "}\n",
    "var filteredDF = convertedDF\n",
    "for (colName <- numericColumns) {\n",
    "  filteredDF = filteredDF.filter(col(colName) >= 0)\n",
    "}\n",
    "\n",
    "filteredDF.persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res14: Long = 26926\n"
     ]
    }
   ],
   "source": [
    "filteredDF.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            label|\n",
      "+-------+-----------------+\n",
      "|  count|            26926|\n",
      "|   mean|4949.703706454728|\n",
      "| stddev|15824.61485217541|\n",
      "|    min|            125.0|\n",
      "|    max|         984109.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredDF.select(\"label\").describe().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Select Features\n",
    "### Simple Correlation-based Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
      "import org.apache.spark.ml.stat.ChiSquareTest\n",
      "with label: 1.0\n",
      "with AGE: NaN\n",
      "with AGE_NEONATE: -4.992307632583083E-4\n",
      "with AMONTH: 0.022599905130392093\n",
      "with AWEEKEND: -0.010414153296519459\n",
      "with DIED: 0.09640357162165143\n",
      "with DQTR: 0.016621095690527915\n",
      "with ELECTIVE: -6.748808916453236E-4\n",
      "with FEMALE: 0.043313805347155614\n",
      "with HCUP_ED: 0.010142957421192722\n",
      "with I10_NDX: NaN\n",
      "with I10_NECAUSE: 0.1482466540380826\n",
      "with I10_NPR: NaN\n",
      "with LOS: 0.8254548216058005\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "\n",
    "for (colName <- numericColumns) {\n",
    "  println(s\"with ${colName}: \" + filteredDF.stat.corr(\"label\", colName))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Understand Selected Features<br>\n",
    "## Nominal Feature: ICD-10CM\n",
    "### Subcategory of Z38\n",
    "\n",
    "1. Z3800: Single liveborn infant, delivered vaginally\n",
    "2. Z3801: Single liveborn infant, delivered by cesarean\n",
    "3. Z3831: Twin liveborn infant, delivered by cesarean\n",
    "4. Z3830: Twin liveborn infant, delivered vaginally\n",
    "5. Z381: Single liveborn infant, born outside hospital\n",
    "6. Z382: Single liveborn infant, unspecified as to place of birth\n",
    "7. Z3862: Triplet liveborn infant, delivered by cesarean\n",
    "8. Z384: Twin liveborn infant, born outside hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|I10_DX1|count|\n",
      "+-------+-----+\n",
      "|  Z3800|19117|\n",
      "|  Z3801| 7328|\n",
      "|  Z3831|  325|\n",
      "|  Z3830|  104|\n",
      "|   Z381|   40|\n",
      "|   Z382|    6|\n",
      "|  Z3862|    5|\n",
      "|   Z384|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredDF.select(\"I10_DX1\").groupBy(\"I10_DX1\").count.sort(desc(\"count\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Nominal Feature: ICD-10-PCS\n",
    "### List of Single Procedure Associated with Z38\n",
    "### Top 5 ICD-10-PCS Procedures\n",
    "1. 0VTTXZZ: Resection of Prepuce, External Approach\n",
    "2. F13Z0ZZ: Hearing Screening Assessment\n",
    "3. 3E0234Z: Introduction of Serum, Toxoid and Vaccine into Muscle, Percutaneous Approach\n",
    "4. F13ZM6Z: Evoked Otoacoustic Emissions, Screening Assessment using Otoacoustic Emission (OAE) Equipment\n",
    "5. F13Z01Z: Hearing Screening Assessment using Audiometer\n",
    "\n",
    "<p><br><br>Some of the other main procedure categories include:</p>\n",
    "\n",
    "- B24: Ultrasonography\n",
    "- 0VJ: Inspection\n",
    "- 4A0: Measurement\n",
    "- 5A0: Assistance\n",
    "- 6A6: Phototherapy\n",
    "- 6A8: Ultraviolet Light Therapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|I10_PR1|count|\n",
      "+-------+-----+\n",
      "|0VTTXZZ|21085|\n",
      "|F13Z0ZZ|2040 |\n",
      "|3E0234Z|1644 |\n",
      "|F13ZM6Z|888  |\n",
      "|F13Z01Z|297  |\n",
      "|3E0134Z|185  |\n",
      "|F13ZL7Z|162  |\n",
      "|3E00X4Z|78   |\n",
      "|F13ZLZZ|72   |\n",
      "|F13ZN6Z|48   |\n",
      "|5A09357|45   |\n",
      "|F13Z3ZZ|40   |\n",
      "|6A600ZZ|40   |\n",
      "|6A601ZZ|38   |\n",
      "|F13ZMZZ|28   |\n",
      "|0VBTXZZ|26   |\n",
      "|3E023GC|17   |\n",
      "|F13ZQKZ|17   |\n",
      "|6A801ZZ|14   |\n",
      "|3E0334Z|13   |\n",
      "|8E0KXY7|13   |\n",
      "|0CN7XZZ|12   |\n",
      "|0VTT0ZZ|11   |\n",
      "|0BH17EZ|8    |\n",
      "|F13Z08Z|7    |\n",
      "|B24DZZZ|7    |\n",
      "|3E0F7GC|6    |\n",
      "|6A800ZZ|5    |\n",
      "|4A03XR1|4    |\n",
      "|F13ZQZZ|4    |\n",
      "|5A09457|4    |\n",
      "|4A05XLZ|4    |\n",
      "|069Y3ZZ|3    |\n",
      "|0BH18EZ|3    |\n",
      "|3E0434Z|2    |\n",
      "|5A0935Z|2    |\n",
      "|0CB7XZZ|2    |\n",
      "|B24BZZZ|2    |\n",
      "|3E0236Z|2    |\n",
      "|06HY33Z|2    |\n",
      "|3E0336Z|2    |\n",
      "|5A09557|2    |\n",
      "|0VTT4ZZ|2    |\n",
      "|0YQA0ZZ|2    |\n",
      "|5A1935Z|2    |\n",
      "|0VNTXZZ|1    |\n",
      "|4A043R1|1    |\n",
      "|03973ZZ|1    |\n",
      "|021Q0JB|1    |\n",
      "|0VJSXZZ|1    |\n",
      "|059Y3ZZ|1    |\n",
      "|5A0955Z|1    |\n",
      "|3E0604Z|1    |\n",
      "|3E02329|1    |\n",
      "|0WQF0ZZ|1    |\n",
      "|5A2204Z|1    |\n",
      "|F13ZNZZ|1    |\n",
      "|10D00Z1|1    |\n",
      "|3E1H38Z|1    |\n",
      "|BT0BYZZ|1    |\n",
      "|5A0945Z|1    |\n",
      "|02SW0ZZ|1    |\n",
      "|0CN70ZZ|1    |\n",
      "|00U107Z|1    |\n",
      "|06H033Z|1    |\n",
      "|0H51XZZ|1    |\n",
      "|009U3ZX|1    |\n",
      "|02UM0JZ|1    |\n",
      "|5A12012|1    |\n",
      "|0B917ZZ|1    |\n",
      "|06H033T|1    |\n",
      "|4A02XFZ|1    |\n",
      "|3E0DX4Z|1    |\n",
      "|3E03329|1    |\n",
      "|04HY33Z|1    |\n",
      "|BT101ZZ|1    |\n",
      "|0CHY7BZ|1    |\n",
      "|5A09358|1    |\n",
      "|02HV33Z|1    |\n",
      "|3E0504Z|1    |\n",
      "|04H033Z|1    |\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredDF.select(\"I10_PR1\").groupBy(\"I10_PR1\").count.sort(desc(\"count\")).show(81, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res20: Long = 81\n"
     ]
    }
   ],
   "source": [
    "filteredDF.select(\"I10_PR1\").distinct.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Nominal Feature: HOSP_DIVISION\n",
    "### Census Division of Hospital\n",
    "- Division 1 (New England): Maine, New Hampshire, Vermont, Massachusetts, Rhode Island, Connecticut\n",
    "- Division 2 (Mid-Atlantic): New York, Pennsylvania, New Jersey\n",
    "- Division 3 (East North Central): Wisconsin, Michigan, Illinois, Indiana, Ohio\n",
    "- Division 4 (West North Central): Missouri, North Dakota, South Dakota, Nebraska, Kansas, Minnesota, Iowa\n",
    "- Division 5 (South Atlantic): Delaware, Maryland, District of Columbia, Virginia, West Virginia, North Carolina, South Carolina, Georgia, Florida\n",
    "- Division 6 (East South Central) Kentucky, Tennessee, Mississippi, Alabama\n",
    "- Division 7 (West South Central) Oklahoma, Texas, Arkansas, Louisiana\n",
    "- Division 8 (Mountain) Idaho, Montana, Wyoming, Nevada, Utah, Colorado, Arizona, New Mexico\n",
    "- Division 9 (Pacific) Alaska, Washington, Oregon, California, Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|HOSP_DIVISION|count|\n",
      "+-------------+-----+\n",
      "|            3| 6494|\n",
      "|            5| 5406|\n",
      "|            2| 4654|\n",
      "|            4| 3483|\n",
      "|            7| 2548|\n",
      "|            6| 1790|\n",
      "|            9| 1057|\n",
      "|            8|  995|\n",
      "|            1|  499|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredDF.select(\"HOSP_DIVISION\").groupBy(\"HOSP_DIVISION\").count.sort(desc(\"count\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Numeric Feature: LOS\n",
    "### Length of Stay\n",
    "<p>Calculated by subtracting the admission date from the discharge date</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               LOS|\n",
      "+-------+------------------+\n",
      "|  count|             26926|\n",
      "|   mean|2.0655500259971773|\n",
      "| stddev| 2.062796005750767|\n",
      "|    min|               0.0|\n",
      "|    max|             182.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredDF.select(\"LOS\").describe().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-----+\n",
      "|I10_DX1|I10_PR1|HOSP_DIVISION|  LOS|\n",
      "+-------+-------+-------------+-----+\n",
      "|  Z3831|5A09557|            5| 38.0|\n",
      "|   Z384|0BH18EZ|            5| 53.0|\n",
      "|  Z3862|04HY33Z|            5| 55.0|\n",
      "|  Z3801|3E0F7GC|            5|112.0|\n",
      "|  Z3831|0BH17EZ|            5| 78.0|\n",
      "|  Z3801|3E0F7GC|            5|182.0|\n",
      "|  Z3801|0VTTXZZ|            5| 46.0|\n",
      "|  Z3801|6A601ZZ|            5| 32.0|\n",
      "|  Z3831|6A601ZZ|            5| 65.0|\n",
      "|  Z3801|06H033T|            2| 90.0|\n",
      "|  Z3831|0YQA0ZZ|            2| 75.0|\n",
      "|  Z3862|02HV33Z|            2| 72.0|\n",
      "|  Z3800|021Q0JB|            2| 38.0|\n",
      "|  Z3800|6A601ZZ|            2| 62.0|\n",
      "|  Z3831|0BH17EZ|            2| 38.0|\n",
      "|  Z3831|6A601ZZ|            2| 59.0|\n",
      "|  Z3801|5A09557|            2| 52.0|\n",
      "+-------+-------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredDF.filter($\"LOS\" > 30).select(\"I10_DX1\",\"I10_PR1\",\"HOSP_DIVISION\",\"LOS\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Prepare Data using Spark Pipeline\n",
    "\n",
    "- indexes nominal features and maps each of those featurse to a binary vector (sparse vector).\n",
    "- combines the list of feature columns into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scala.collection.mutable.ArrayBuffer\n",
      "stages: scala.collection.mutable.ArrayBuffer[org.apache.spark.ml.PipelineStage] = ArrayBuffer()\n",
      "charColumns: Array[String] = Array(I10_DX1, I10_PR1, HOSP_DIVISION)\n",
      "featureColumns: Array[String] = Array(LOS, I10_DX1_ENC, I10_PR1_ENC, HOSP_DIVISION_ENC)\n",
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_51844443de59\n",
      "res33: scala.collection.mutable.ArrayBuffer[org.apache.spark.ml.PipelineStage] = ArrayBuffer(strIdx_9f05a57541ae, oneHotEncoder_488bc5503659, strIdx_8c860ec532a1, oneHotEncoder_51926bfde7c9, strIdx_5d6f7f011aa8, oneHotEncoder_940669a4f1d3, vecAssembler_51844443de59)\n"
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "// array of Spark Pipeline Stages\n",
    "var stages = ArrayBuffer[PipelineStage]()\n",
    "\n",
    "// transforms nominal features\n",
    "val charColumns = Array(\"I10_DX1\",\"I10_PR1\",\"HOSP_DIVISION\")\n",
    "for(colName <- charColumns) {\n",
    "  var indexer = new StringIndexer()\n",
    "    .setInputCol(colName)\n",
    "    .setOutputCol(colName+\"_IND\")\n",
    "  var encoder = new OneHotEncoderEstimator()\n",
    "    .setInputCols(Array(indexer.getOutputCol))\n",
    "    .setOutputCols(Array(colName+\"_ENC\"))\n",
    "    .setHandleInvalid(\"keep\")\n",
    "  \n",
    "  stages += indexer\n",
    "  stages += encoder\n",
    "}\n",
    "\n",
    "// final feature columns\n",
    "val featureColumns = Array(\"LOS\",\"I10_DX1_ENC\",\"I10_PR1_ENC\",\"HOSP_DIVISION_ENC\")\n",
    "\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(featureColumns).\n",
    "    setOutputCol(\"features\")\n",
    "\n",
    "stages += assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_46c3cabd7a3a\n",
      "preparedDF: org.apache.spark.sql.DataFrame = [AGE: double, AGE_NEONATE: double ... 103 more fields]\n"
     ]
    }
   ],
   "source": [
    "val pipeline = new Pipeline().setStages(stages.toArray)\n",
    "val preparedDF = pipeline.fit(filteredDF).transform(filteredDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------+\n",
      "|label |features                         |\n",
      "+------+---------------------------------+\n",
      "|5933.0|(99,[0,2,9,90],[3.0,1.0,1.0,1.0])|\n",
      "|4641.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|4839.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|4179.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|4179.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|2329.0|(99,[0,1,9,90],[1.0,1.0,1.0,1.0])|\n",
      "|4259.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|4203.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|4135.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|4647.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|4179.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|2650.0|(99,[0,1,9,90],[1.0,1.0,1.0,1.0])|\n",
      "|2444.0|(99,[0,1,9,90],[1.0,1.0,1.0,1.0])|\n",
      "|4659.0|(99,[0,2,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|4573.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|4683.0|(99,[0,1,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|4675.0|(99,[0,2,9,90],[2.0,1.0,1.0,1.0])|\n",
      "|2440.0|(99,[0,1,9,90],[1.0,1.0,1.0,1.0])|\n",
      "|2222.0|(99,[0,1,9,90],[1.0,1.0,1.0,1.0])|\n",
      "|3506.0|(99,[0,2,9,90],[2.0,1.0,1.0,1.0])|\n",
      "+------+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDF.select(\"label\",\"features\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Check String Index and Encoded Vector for Nominal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+\n",
      "|I10_DX1|I10_DX1_IND|  I10_DX1_ENC|\n",
      "+-------+-----------+-------------+\n",
      "|   Z382|        5.0|(8,[5],[1.0])|\n",
      "|  Z3862|        6.0|(8,[6],[1.0])|\n",
      "|   Z381|        4.0|(8,[4],[1.0])|\n",
      "|  Z3800|        0.0|(8,[0],[1.0])|\n",
      "|  Z3801|        1.0|(8,[1],[1.0])|\n",
      "|   Z384|        7.0|(8,[7],[1.0])|\n",
      "|  Z3831|        2.0|(8,[2],[1.0])|\n",
      "|  Z3830|        3.0|(8,[3],[1.0])|\n",
      "+-------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDF.select(\"I10_DX1\",\"I10_DX1_IND\",\"I10_DX1_ENC\").dropDuplicates.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------------+\n",
      "|I10_PR1|I10_PR1_IND|    I10_PR1_ENC|\n",
      "+-------+-----------+---------------+\n",
      "|F13ZL7Z|        6.0| (81,[6],[1.0])|\n",
      "|059Y3ZZ|       67.0|(81,[67],[1.0])|\n",
      "|5A0955Z|       78.0|(81,[78],[1.0])|\n",
      "|009U3ZX|       48.0|(81,[48],[1.0])|\n",
      "|3E023GC|       17.0|(81,[17],[1.0])|\n",
      "|10D00Z1|       63.0|(81,[63],[1.0])|\n",
      "|5A09457|       31.0|(81,[31],[1.0])|\n",
      "|00U107Z|       76.0|(81,[76],[1.0])|\n",
      "|0VNTXZZ|       69.0|(81,[69],[1.0])|\n",
      "|3E02329|       61.0|(81,[61],[1.0])|\n",
      "|3E0504Z|       54.0|(81,[54],[1.0])|\n",
      "|F13ZNZZ|       53.0|(81,[53],[1.0])|\n",
      "|4A02XFZ|       73.0|(81,[73],[1.0])|\n",
      "|5A09358|       72.0|(81,[72],[1.0])|\n",
      "|8E0KXY7|       19.0|(81,[19],[1.0])|\n",
      "|5A2204Z|       66.0|(81,[66],[1.0])|\n",
      "|0H51XZZ|       49.0|(81,[49],[1.0])|\n",
      "|4A03XR1|       28.0|(81,[28],[1.0])|\n",
      "|5A12012|       74.0|(81,[74],[1.0])|\n",
      "|0VBTXZZ|       15.0|(81,[15],[1.0])|\n",
      "+-------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDF.select(\"I10_PR1\",\"I10_PR1_IND\",\"I10_PR1_ENC\").dropDuplicates.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-----------------+\n",
      "|HOSP_DIVISION|HOSP_DIVISION_IND|HOSP_DIVISION_ENC|\n",
      "+-------------+-----------------+-----------------+\n",
      "|            2|              2.0|    (9,[2],[1.0])|\n",
      "|            6|              5.0|    (9,[5],[1.0])|\n",
      "|            7|              4.0|    (9,[4],[1.0])|\n",
      "|            8|              7.0|    (9,[7],[1.0])|\n",
      "|            9|              6.0|    (9,[6],[1.0])|\n",
      "|            3|              0.0|    (9,[0],[1.0])|\n",
      "|            4|              3.0|    (9,[3],[1.0])|\n",
      "|            5|              1.0|    (9,[1],[1.0])|\n",
      "|            1|              8.0|    (9,[8],[1.0])|\n",
      "+-------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDF.select(\"HOSP_DIVISION\",\"HOSP_DIVISION_IND\",\"HOSP_DIVISION_ENC\").dropDuplicates.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Split Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomSplitDSs: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([label: double, features: vector], [label: double, features: vector])\n",
      "trainingDS: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
      "testingDS: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
     ]
    }
   ],
   "source": [
    "val randomSplitDSs: Array[Dataset[Row]] = preparedDF.select(\"label\",\"features\").randomSplit(Array(0.7, 0.3), 11)\n",
    "val trainingDS: Dataset[Row] = randomSplitDSs(0)\n",
    "val testingDS: Dataset[Row] = randomSplitDSs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDS.select(\"label\",\"features\").\n",
    "  limit(100).\n",
    "  write.\n",
    "  format(\"libsvm\").\n",
    "  option(\"header\", \"true\").\n",
    "  mode(\"overwrite\").\n",
    "  save(\"s3://dalin-ml-pipeline/testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Train and Build SageMaker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import com.amazonaws.services.sagemaker.sparksdk.{CustomNamePolicyFactory, EndpointCreationPolicy, IAMRole, S3DataPath, SageMakerModel}\n",
      "import com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostSageMakerEstimator\n",
      "import java.time.LocalDateTime\n",
      "import java.time.format.DateTimeFormatter\n",
      "uid: String = 201911271926\n",
      "xgboostEstimator: com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostSageMakerEstimator = sagemaker_c7d3a501ffcb\n",
      "res41: xgboostEstimator.type = sagemaker_c7d3a501ffcb\n",
      "res42: xgboostEstimator.type = sagemaker_c7d3a501ffcb\n"
     ]
    }
   ],
   "source": [
    "import com.amazonaws.services.sagemaker.sparksdk.{CustomNamePolicyFactory, EndpointCreationPolicy, IAMRole, S3DataPath, SageMakerModel}\n",
    "import com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostSageMakerEstimator\n",
    "\n",
    "import java.time.LocalDateTime\n",
    "import java.time.format.DateTimeFormatter\n",
    "val uid = DateTimeFormatter.ofPattern(\"yyyyMMddHHmm\").format(LocalDateTime.now)\n",
    "\n",
    "val xgboostEstimator = new XGBoostSageMakerEstimator(\n",
    "  sagemakerRole=IAMRole(sageMakerRoleArn),\n",
    "  trainingInstanceType = \"ml.m5.xlarge\",\n",
    "  trainingInstanceCount = 1,\n",
    "  endpointInstanceType = \"ml.m5.xlarge\",\n",
    "  endpointInitialInstanceCount = 1,\n",
    "  trainingInputS3DataPath = S3DataPath(s3BucketName, sageMakerInputPrefix),\n",
    "  trainingOutputS3DataPath = S3DataPath(s3BucketName, sageMakerOutputPrefix),\n",
    "  endpointCreationPolicy = EndpointCreationPolicy.CREATE_ON_CONSTRUCT, // DO_NOT_CREATE\n",
    "  namePolicyFactory = new CustomNamePolicyFactory(s\"Z38-training-$uid\",\n",
    "                                                  s\"Z38-model-$uid\",\n",
    "                                                  \"Z38-endpointConfig\",\n",
    "                                                  \"Z38-endpoint\")\n",
    ")\n",
    "xgboostEstimator.setNumRound(15)\n",
    "xgboostEstimator.setObjective(\"reg:linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: com.amazonaws.services.sagemaker.sparksdk.SageMakerModel = sagemaker_c7d3a501ffcb\n"
     ]
    }
   ],
   "source": [
    "val model: SageMakerModel = xgboostEstimator.fit(trainingDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Evaluate Model Performance\n",
    "### Using Spark RegressionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]\n",
      "predictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[282] at rdd at <console>:57\n"
     ]
    }
   ],
   "source": [
    "val predictions: DataFrame = model.transform(testingDS)\n",
    "// predictions.show(50)\n",
    "val predictionAndLabels: RDD[(Double, Double)] = predictions.select($\"label\",$\"prediction\").as[(Double, Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
      "metrics: org.apache.spark.mllib.evaluation.RegressionMetrics = org.apache.spark.mllib.evaluation.RegressionMetrics@37c2c797\n",
      "MSE = 5.003855044569909E7\n",
      "RMSE = 7073.793214796365\n",
      "R-squared = 0.7833784302303666\n",
      "MAE = 1767.2243561865569\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "\n",
    "// Instantiate metrics object\n",
    "val metrics = new RegressionMetrics(predictionAndLabels)\n",
    "\n",
    "// Squared error\n",
    "println(s\"MSE = ${metrics.meanSquaredError}\")\n",
    "println(s\"RMSE = ${metrics.rootMeanSquaredError}\")\n",
    "\n",
    "// R-squared\n",
    "println(s\"R-squared = ${metrics.r2}\")\n",
    "\n",
    "// Mean absolute error\n",
    "println(s\"MAE = ${metrics.meanAbsoluteError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up SageMaker Resources\n",
    "#### Deletes SageMakerModel, Endpoint Configuration, and Endpoint created by the SageMakerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import com.amazonaws.services.sagemaker.sparksdk.SageMakerResourceCleanup\n",
      "resource_cleanup: com.amazonaws.services.sagemaker.sparksdk.SageMakerResourceCleanup = com.amazonaws.services.sagemaker.sparksdk.SageMakerResourceCleanup@2b3c0cb1\n"
     ]
    }
   ],
   "source": [
    "import com.amazonaws.services.sagemaker.sparksdk.SageMakerResourceCleanup\n",
    "\n",
    "val resource_cleanup = new SageMakerResourceCleanup(model.sagemakerClient)\n",
    "resource_cleanup.deleteResources(model.getCreatedResources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
